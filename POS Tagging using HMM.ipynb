{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "\n",
    "    with open(path, 'r') as f:   \n",
    "        data = f.readlines()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING DATA TO HANDLE OOV WORDS AND MAKE A VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(data):\n",
    "    \n",
    "    words = []\n",
    "    for w in data:\n",
    "        tup = w.split()\n",
    "        if len(tup) == 2:\n",
    "            words.append(tup[0].lower())\n",
    "            \n",
    "    return words\n",
    "\n",
    "def make_vocab(words):\n",
    "    \n",
    "    word_count = dict(Counter(words))\n",
    "    \n",
    "    #vocab consists of all words which appear atleast twice in the training corpus\n",
    "    vocab = [w for w, freq in word_count.items() if (freq > 1 and w != '\\n')]  \n",
    "    \n",
    "    #adding new words to vocab which will be used to classify words which are not seen before\n",
    "    unk_list = [\"--unk--\", \"--n--\"]\n",
    "    vocab += unk_list\n",
    "    vocab = sorted(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def get_word_tag(line, vocab):\n",
    "    \n",
    "    # If line is empty return placeholders for word and tag\n",
    "    if not line.split():\n",
    "        word = \"--n--\"  #word indicating new line\n",
    "        tag = \"--s--\"   #tag used to indicate transition from initial state(starting of sentence) to a particular state in the HMM.\n",
    "    else:\n",
    "        word, tag = line.split()\n",
    "        word = word.lower()\n",
    "        if word not in vocab: \n",
    "            word = \"--unk--\"  #unknown word tag\n",
    "            \n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data('WSJ_02-21.pos')\n",
    "words = get_words(data)\n",
    "vocab = make_vocab(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING DICTIONARIES DEPICTING COUNTS OF VARIOUS TAG AND WORD COMBINATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_dicts(data, vocab):\n",
    "    \n",
    "    prev_tag = '--s--'\n",
    "    transition_counts = defaultdict(int)\n",
    "    emission_counts = defaultdict(int)\n",
    "    tags = defaultdict(int)\n",
    "    \n",
    "    for line in data:\n",
    "        w, t = get_word_tag(line, vocab)\n",
    "        transition_counts[(prev_tag, t)] += 1\n",
    "        emission_counts[(t, w)] += 1\n",
    "        tags[t] += 1\n",
    "        prev_tag = t\n",
    "    \n",
    "    return transition_counts, emission_counts, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition_counts, emission_counts, tags = make_dicts(data, vocab)\n",
    "# (Takes time to complete, thus, these dictionaries have been calculated and stored in files)\n",
    "\n",
    "def save_dict(file, filename):\n",
    "    \n",
    "    title = filename + '.pkl'\n",
    "    f = open(title, 'wb')\n",
    "    pk.dump(file, f)\n",
    "    f.close()\n",
    "\n",
    "def load_dict(filename):\n",
    "    \n",
    "    title = filename + '.pkl'\n",
    "    f = open(title, 'rb')\n",
    "    return pk.load(f)\n",
    "\n",
    "'''save_dict(transition_counts, 'transition_counts')    #already saved\n",
    "save_dict(emission_counts, 'emission_counts')\n",
    "save_dict(tags, 'tags')'''\n",
    "\n",
    "#loading dictionaries \n",
    "transition_counts = load_dict('transition_counts')\n",
    "emission_counts = load_dict('emission_counts')\n",
    "tags = load_dict('tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_words(vocab):\n",
    "    word_to_idx = {}\n",
    "    for i, word in enumerate(vocab):\n",
    "        word_to_idx[word] = i\n",
    "    \n",
    "    return word_to_idx\n",
    "\n",
    "def dict_tags(tags):\n",
    "    tag_to_idx = {}\n",
    "    all_tags = sorted(tags.keys())\n",
    "    for i, tag in enumerate(all_tags):\n",
    "        tag_to_idx[i] = tag\n",
    "        \n",
    "    return tag_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING THE TRANSITION AND EMISSION MATRICES FOR THE HMM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrices(transition_counts, emission_counts, tags, vocab):\n",
    "    \n",
    "    all_tags = sorted(tags.keys())\n",
    "    m = len(all_tags) #no. of tags\n",
    "    n = len(vocab) #no of words\n",
    "    \n",
    "    A = np.zeros((m, m))\n",
    "    B = np.zeros((m, n))\n",
    "    \n",
    "    for i in range(len(all_tags)):\n",
    "        for j in range(len(tags.keys())):\n",
    "            t1 = all_tags[i]\n",
    "            t2 = all_tags[j]\n",
    "            num = 1\n",
    "            \n",
    "            if (t1, t2) in transition_counts.keys():\n",
    "                num = transition_counts[(t1, t2)] + 1\n",
    "            denom = tags[t1] + m\n",
    "            \n",
    "            A[i, j] = num / denom\n",
    "        \n",
    "    for i in range(len(all_tags)):\n",
    "        for j in range(len(vocab)):\n",
    "            tag = all_tags[i]\n",
    "            word = vocab[j]\n",
    "            num = 1\n",
    "            if (tag, word) in emission_counts.keys():\n",
    "                num = emission_counts[(tag, word)] + 1\n",
    "            denom = tags[tag] + n\n",
    "            \n",
    "            B[i, j] = num / denom\n",
    "            \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = dict_words(vocab)\n",
    "idx_to_tag = dict_tags(tags)\n",
    "transition_matrix, emission_matrix = make_matrices(transition_counts, emission_counts, tags, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VITERBI ALGORITHM (FOR PREDICTION OF POS TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_initialization(sentence, A, B, tags):\n",
    "    \n",
    "    num_tags = A.shape[0]\n",
    "    all_tags = sorted(tags.keys())\n",
    "    k = len(sentence)\n",
    "    probs = np.zeros((num_tags, k))\n",
    "    states = np.zeros((num_tags, k), dtype = int)\n",
    "    start_idx = all_tags.index('--s--')\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        if A[start_idx, i] == 0:\n",
    "            probs[i, 0] = float('-inf')  #log0\n",
    "        else:\n",
    "            probs[i, 0] = np.log(A[start_idx, i]) + np.log(B[i, word_to_idx[sentence[0]]])\n",
    "        \n",
    "        states[i, 0] = start_idx\n",
    "        \n",
    "    return probs, states\n",
    "\n",
    "def viterbi_forward(probs, states, A, B, sentence, word_to_idx):\n",
    "    \n",
    "    num_tags = A.shape[0]\n",
    "    \n",
    "    for i in range(1, len(sentence)):\n",
    "        for j in range(num_tags):\n",
    "            max_prob = float('-inf')\n",
    "            best_state = None\n",
    "            for k in range(num_tags):\n",
    "                prob = probs[k, i - 1] + math.log(A[k, j]) + math.log(B[j, word_to_idx[sentence[i]]])\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_state = k\n",
    "            \n",
    "            probs[j, i] = max_prob\n",
    "            states[j, i] = best_state\n",
    "            \n",
    "\n",
    "def viterbi_backward(probs, states, sentence, idx_to_tag):\n",
    "    \n",
    "    k = len(sentence)\n",
    "    pos = [None] * k\n",
    "    idx = [None] * k\n",
    "    num_tags = probs.shape[0]\n",
    "    best_prob = float('-inf')\n",
    "    prev_state = None\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        prob = probs[i, k - 1]\n",
    "        if prob > best_prob:\n",
    "            best_prob = prob\n",
    "            idx[k - 1] = i\n",
    "            \n",
    "    pos[k - 1] = idx_to_tag[idx[k - 1]]\n",
    "    \n",
    "    for i in range(k - 1, -1 , -1):\n",
    "        prev_tag = idx[i]\n",
    "        idx[i - 1] = states[prev_tag, i]\n",
    "        pos[i - 1] = idx_to_tag[idx[i - 1]]\n",
    "        \n",
    "    return pos\n",
    "\n",
    "def predictPOS(transition_matrix, emission_matrix, sentence, tags, word_to_idx, idx_to_tag):\n",
    "    \n",
    "    probs, states = viterbi_initialization(sentence, transition_matrix, emission_matrix, tags)\n",
    "    viterbi_forward(probs, states, transition_matrix, emission_matrix, sentence, word_to_idx)\n",
    "    pred = viterbi_backward(probs, states, sentence, idx_to_tag)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def accuracy(pred, test_tags):\n",
    "    \n",
    "    c = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == test_tags[i]:\n",
    "            c += 1\n",
    "    return (c / len(pred)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_data('WSJ_24.pos')\n",
    "test_words = []\n",
    "test_tags = []\n",
    "\n",
    "for line in test:\n",
    "    word, tag = get_word_tag(line, vocab)\n",
    "    test_words.append(word)\n",
    "    test_tags.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.18983011199158"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predictPOS(transition_matrix, emission_matrix, test_words, tags, word_to_idx, idx_to_tag)\n",
    "accuracy(pred, test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTING FOR USER-INPUT SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter sentence : I believe the predictions made are good. You can try for yourself!\n"
     ]
    }
   ],
   "source": [
    "s = input('Enter sentence : ')\n",
    "user_words = word_tokenize(s)\n",
    "user_tests = []\n",
    "for word in user_words:\n",
    "    word = word.lower()\n",
    "    if word not in vocab:\n",
    "        user_tests.append('--unk--')\n",
    "    else:\n",
    "        user_tests.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I : PRP\n",
      "believe : VBP\n",
      "the : DT\n",
      "predictions : NNS\n",
      "made : VBN\n",
      "are : VBP\n",
      "good : JJ\n",
      ". : .\n",
      "You : PRP\n",
      "can : MD\n",
      "try : VB\n",
      "for : IN\n",
      "yourself : PRP\n",
      "! : --s--\n"
     ]
    }
   ],
   "source": [
    "pos_tags = predictPOS(transition_matrix, emission_matrix, user_tests, tags, word_to_idx, idx_to_tag)\n",
    "\n",
    "for i in range(len(user_words)):\n",
    "    print(user_words[i] + ' : ' + pos_tags[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
